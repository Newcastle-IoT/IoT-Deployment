##################################################################################################################
##																												##
## Spark worker(s) for running Spark in a standalone mode. Run the following commands to build the image and    ##
## run the worker container(s). These the worker(s) is intended to be run as service on a Swarm mode cluster.	##
##																			  									##
## $docker build -t <image name> --rm <location of this file>													##
##															  													##
## $docker service create --name worker --mode global --network <your overlay net> -p 8081:8081 <image name> 	##
## <master node public DNS>   																					##
##	  	                                            															##
##################################################################################################################

FROM alisu/java:latest
MAINTAINER Saleh Mohamed <s.mohamed@nclf.ac.uk>
 
LABEL "hadoop.version"="2.7.1", "spark.version"="2.0.0"
ENV home /user 
WORKDIR ${home}
ADD scripts/worker-config.sh ${home}
RUN mkdir Downloads

## Download and install hadoop
RUN wget http://apache.mirrors.tds.net/hadoop/common/hadoop-2.7.1/hadoop-2.7.1.tar.gz -P Downloads
RUN tar zxvf Downloads/hadoop-* -C /usr/local
RUN mv /usr/local/hadoop-* /usr/local/hadoop
ENV JAVA_HOME=/usr
ENV PATH $PATH:$JAVA_HOME/bin
ENV HADOOP_HOME /usr/local/hadoop
ENV PATH $PATH:$HADOOP_HOME/bin
ENV HADOOP_CONF_DIR /usr/local/hadoop/etc/hadoop
RUN chown -R root $HADOOP_HOME

## Install Scala
RUN apt-get update -y
RUN apt-get install scala -y

## Download and install Sparks
RUN wget http://d3kbcqa49mib13.cloudfront.net/spark-2.0.0-bin-hadoop2.7.tgz -P Downloads
RUN tar zxvf Downloads/spark-* -C /usr/local
RUN mv /usr/local/spark-* /usr/local/spark
ENV SPARK_HOME /usr/local/spark
ENV PATH $PATH:$SPARK_HOME/bin
ENV PATH $PATH:$SPARK_HOME/sbin
RUN chown -R root $SPARK_HOME
RUN cp $SPARK_HOME/conf/spark-env.sh.template $SPARK_HOME/conf/spark-env.sh
RUN echo "export JAVA_HOME=/usr" | tee -a $SPARK_HOME/conf/spark-env.sh
RUN echo "export SPARK_WORKER_CORES=6" | tee -a $SPARK_HOME/conf/spark-env.sh

EXPOSE 8080
EXPOSE 8081
EXPOSE 4040
EXPOSE 7077
EXPOSE 6066
ENTRYPOINT ["/bin/bash", "worker-config.sh"]

